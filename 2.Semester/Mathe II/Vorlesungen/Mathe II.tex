\documentclass{article}
\usepackage{mhchem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[
  left=1cm,
  right=1cm,
  top=2cm,
  bottom=2cm,
]{geometry}

\begin{document}

\section{Lineare Algebra}
\subsection{Lineare Vektorräume}
Beispiel für Vektoren mit mehr als Komponenten\\
\hspace*{1cm} Quantenmechanik:\\
\hspace*{1cm} MO-LCAO: \underline{M}olecular \underline{O}rbitals by \underline{L}inear \underline{C}ombination of \underline{A}tomic \underline{O}rbitals\\
Das bindende $\sigma$-MO im \ce{H2} ist
\begin{equation*}
    |\sigma\rangle =c_1(|1s_A+|1s_B\rangle )+c_2(|2s_A\rangle +|2s_B\rangle )+c_3(|2p_{zA}\rangle +\dots)
\end{equation*}
MO-Koeffizientenvektor: $(c_1,c_2,c_3,c_4,\dots)$
Def.: Eine Menge V von Vektoren {${\vec{a},\vec{b},\dots}$} heißen linearer Vektorraum, wenn gilt:
\vspace*{1cm}
\begin{itemize}
    \item Für alle Elemente $\vec{a}$, $\vec{b}$ Element von V gibt es genau ein Element $\vec{a} + \vec{b}$ Element von V (Die Summe zweier Elemente von V gehört wieder zu V)
    \item Assoziativgesetz: $\vec{a}+(\vec{b}+\vec{c}) = (\vec{a}+\vec{b})+\vec{c}$ für $\vec{a}, \vec{b}, \vec{c}$ Element von V
    \item Kommutativgesetz: $\vec{a}+\vec{b} = \vec{b} + \vec{a}$
    \item Neutrales Element "Nullstelle $\vec{o}$ Element von V: $\vec{a} + \vec{o} = \vec{a}$
    \item Inverses Element $-\vec{a}$: $\vec{a} + (-\vec{a}) = 0$
\end{itemize}
Multiplikation mit reellen/komplexen Zahlen
\begin{itemize}
    \item Für alle Elemente $\vec{a}$ Element V und z Element C gibt es genau ein Element $z\cdot\vec{a}$ Element V
    \item Assoziativgesetz
    \item Distributivgesetz
\end{itemize}
\subsection{Dimension und Basis eines Vektorraums}
Def.: Die Vektoren {$\vec{a_1},\dots,\vec{a_n}$} heißen linear unabhängig, wenn $\sum_{k=1}^{n} z_k \vec{a_k}=\vec{o}$ nur für $\vec{z_k} = 0 (k=1,\dots,\dots)$ erfüllbar ist\\
Ist mindestens ein $z_k \neq 0$ wählbar, so heißen die Vektoren linear abhängig, d.h. ein Vektor lässt sich als Linearkombination der anderen schreiben:\\
$\vec{a_k}=-\frac{1}{z_k}\sum_{l=1, l\neq k}^{n}z_l \vec{a_l} = \sum_{l=1,l\neq k}^{a}{z_b}\vec{a_l}$ \hspace*{1cm} $z_b=-\frac{z_l}{z_k}$\\
Beispiel
\begin{equation*}
    \left[\left(\begin{array}{c}
        0\\ 1\\ 1
    \end{array}\right),\left(\begin{array}{c}
        1\\ 0\\ 1
    \end{array}\right),\left(\begin{array}{c}
        1\\ 1\\ 0
    \end{array}\right)\right]
\end{equation*}
\begin{equation*}
    z_1\left(\begin{array}{c}
        0\\ 1\\ 1
    \end{array}\right)+z_2\left(\begin{array}{c}
        1\\ 0\\ 1
    \end{array}\right)+z_3\left(\begin{array}{c}
        1\\ 1\\ 0
    \end{array}\right) = \vec{o}
\end{equation*}
\begin{equation*}
    z_1 0+z_2 0 + z_3 1 = 0
\end{equation*}
\begin{equation*}
    z_1 1+z_2 0 + z_3 1 = 0
\end{equation*}
\begin{equation*}
    z_1 1 +z_2 1 + z_3 0 = 0
\end{equation*}
Auflösen nach $z_1,z_2,z_3$ durch Addition bzw. Subtraktion von Vielfachen der Zeilen (Einzelgleichungen) bis Dreiecksform erreicht wurde, anschließend $z_1,z_2,z_3$ durch Rück-rechnung ermitteln, dies ist der Gaußsche Algorithmus\\
Hierbei gibt es 4 Möglichkeiten, dies ist eine Lösung:\\
\begin{equation*}
    0, 0, 2 | 0
\end{equation*}\
\begin{equation*}
    0, -1, 1 | 0
\end{equation*}
\begin{equation*}
    1, 1, 0 | 0
\end{equation*}
Durch Rückrechnung:
\begin{equation*}
    z_3 = 0, z_2 = 0, z_1 = 0
\end{equation*}
Die Vektoren sind linear unabhängig voneinandern d.h. (hier) sie lieben nicht in einer Ebene aber auf einer Geraden!\\
Das von den Vektoren aufgespannte Spat hat ein Volumen $\neq$ 0, d.h. die Detrminante ist ungleich 0, hier ergibt die Determinante 2.\\
Das Spatprodukt geht nur mit 3 3D-Vektoren, die allgemeine Determinantenmethode geht mit $n$ $n$D-Vektoren\\
Entstünde beim LGS eine Mullzeile, kann ein Parameter frei gewählt werden, hierbei kann man diesen z.B. durch $\lambda$ ersetzen.\
Da i.A. $\lambda \neq 0$ gewählt werden darf, sind die Vektoren linear abhängig, d.h. die liegen in einer Ebene,\\
$->$ Das von den Vektoren Aufgespannte Spat hat ein Volumen = 0, d.h. die Determinante würde hierbei 0 ergeben.\\
\\
Linear abhängig heißt, dass einige Vektoren durch andere desselben Satzes ausdrückbar sind.\\
\begin{equation*}
\left(\begin{array}{c}
    0 \\ i \\ -i
\end{array}\right)=-i\left[\left(\begin{array}{c}
    -1 \\ 0 \\ 1
\end{array}\right)+\left(\begin{array}{c}
    1 \\ -1 \\ 0
\end{array}\right)\right]
\end{equation*}
Def.: Die Maximalzahl linearunabhängiger Vektoren in der Menge V heißt Dimension des Vektorraums.\\
\\
Beispiel:\\
\begin{center}
    \begin{tabular}{c c} 
     \hline
     Vorstellung & Dimension \\ 
     \hline
     Punkt & 0\\ 
     Gerade & 1 \\ 
     Ebene & 2\\
     Raum & 3\\
     \hline
    \end{tabular}
    \end{center}
$->$ Die Vektoren ${\vec{o},\vec{a}}$ sind immer linear abhängig.\\
$->$ m $>$ n n-dimensionale Vektore sind immer linear abhängig.\\
Def.: Eine Menge B, die die Maximalzahl lineat unabhängiger Vektoren in der Menge V enthält, heißt Basis des Vekotrraumes V. Jeder Vektor $\vec{a_i}$ Element in V lässt sich als Linearkombination der Basisvektoren $\vec{b_j}$ Element in B c V. darstellen:\\
\begin{equation*}
    \vec{a_i}=\sum_{k=1}^{n}\alpha_{ki} \vec{b_k} = \left(\alpha_{1i},\alpha_{2i},\alpha_{3i},\dots,\alpha_{ni}\right)
\end{equation*}
Dies ist die Komponentendarstellung\\
\\
Anmerkung: Im allgemeinen gibt es mehr als nur eine mögliche Basis.\\Aber: Die Anzahl der linear unabhängigen Basisvektoren ist immer gleich der Dimensionalität.\\
\\
\\
Beispiel: 3D-Raum\\
Basis:\\
\begin{equation*}
    i = \left(\begin{array}{c}
        1 \\ 0 \\ 0
    \end{array}\right), j = \left(\begin{array}{c}
        0 \\ 1 \\ 0
    \end{array}\right), k = \left(\begin{array}{c}
        0 \\ 0 \\ 1
    \end{array}\right)
\end{equation*}
Beliebiger Vektor darstellbar durch
\begin{equation*}
    \vec{a_i}=\sum_{k=1}^{n}\alpha_{ki}\vec{b_k}
\end{equation*}
Zum Beispiel:
\begin{equation*}
    \left(\begin{array}{c}
        3 \\ 1 \\ -1
    \end{array}\right) = 3i + j - k
\end{equation*}
Diese abkürzende Schreibweise gibt nur die Koeffizienten $\alpha_{ki}$ an, nicht die Basisvektoren. Sie macht aber ohne Festlegung der Basisvektoren keinen Sinn, denn in anderer Basis sind die Koeffizienten im Allgemeinen anders.\\
\\
Neue Basis (ausgedrückt in alter Basis i, j, k)
\begin{equation*}
    \vec{l}=\left(\begin{array}{c}
        1 \\ 1 \\ 0
    \end{array}\right)_{ijk} = i + j 
\end{equation*}
\begin{equation*}
    \vec{m}=\left(\begin{array}{c}
        1 \\ -1 \\ 0
    \end{array}\right)_{ijk} = i - j
\end{equation*}
\begin{equation*}
    \vec{n}=\left(\begin{array}{c}
        0 \\ 0 \\ -1
    \end{array}\right)_{ijk} = -k
\end{equation*}
Selber Vektor von oben in neuer Basis:
\begin{equation*}
    \left(\begin{array}{c}
        3 \\ 1 \\ -1
    \end{array}\right)_{ijk} = 2 \left(\begin{array}{c}
        1 \\ 1 \\ 0
    \end{array}\right)_{ijk} + \left(\begin{array}{c}
        1 \\ -1 \\ 0
    \end{array}\right)_{ijk} + \left(\begin{array}{c}
        0 \\ 0 \\ -1
    \end{array}\right)_{ijk}
\end{equation*}
\begin{equation*}
    \left(\begin{array}{c}
        3 \\ 1 \\ -1
    \end{array}\right)_{ijk} = 2 \vec{l} + \vec{m} + \vec{n} = \left(\begin{array}{c}
        2 \\ 1 \\ 1
    \end{array}\right)_{lmn} 
\end{equation*}
Beispiel: $\left\{1,x,x^2,\dots\right\}$ bilden die Basis der unendlich dimensionalen Vektorraums der Polynome.\
\begin{equation*}
    P_5\left(x\right)=7+3x^2+4x^3-x^5
\end{equation*}
ist darstellbar als
\begin{equation*}
    \left(7,0,3,4,0,-1\right)
\end{equation*}
Dies sind die jeweiligen Koeffizienten.\\
Def.: Eine Teilmenge von Vektoren eines Vektorraums $\mathbb{V}$ bildet einen Untervektorraum $\mathbb{U}$, wenn alle Kriterien einer Vektorraumes erfüllt werden.\\
Trivial: $\left\{\vec{o}\right\}$ sowie $\mathbb{V}$ sind Untervektorräume von $\mathbb{V}$.\\
Beispiel:\\
Gerade ist eindimensionaler Unterraum einer Ebene.
Ebene ist zweidimensionaler Unterraum des 3D-Raums.
\subsection{Spezielle lineare Vektorräume}
\subsubsection{Euklidische Vektorräume}
a) Def.: Eine mathematische Operation $\varphi$, die zwei Vektoren eines Vektorraums $\mathbb{V}$ eine reelle Zahl zuordnet, heißt Skalarprodukt, wenn gilt:\\
für $\vec{a},\vec{b},\vec{c} \in \mathbb{V}$\\
\begin{itemize}
    \item[1)] Distributivgesetz
    \item[2)] Kommutativgesetz
    \item[3)] Homogenität $x \in \mathbb{R}$\\\\
    $\varphi\left(x\vec{a},\vec{b}\right)=\varphi\left(\vec{a},x\vec{b}\right)=x\varphi\left(\vec{a},\vec{b}\right)$
    \item positive Definitheit\\\\
    $\varphi\left(\vec{a},\vec{a}\right) > 0$ für $\vec{a} \neq \vec{0}$
\end{itemize}
Beispiel: Skalarprodukt zwischen
\begin{equation*}
    \vec{a}=\left(x_a,y_a\right),\vec{b}=\left(x_b,y_b\right)
\end{equation*}
\begin{equation*}
    => \vec{a}\vec{b}=x_ax_b+y_ay_b
\end{equation*}
Beispiel Skalarprodukt zwischen $|a\rangle =a(x), |b\rangle =b(x)$
\begin{equation*}
    => \varphi\left(a,b\right) = <a|b\rangle =\int a(x)b(x)dx
\end{equation*}
\\
b) Def.: Die Norm (Länge) eines Vektors $\vec{a}\in\mathbb{V}$ ist von der Definition her:
\begin{equation*}
    ||\vec{a}|| = \sqrt{\varphi\left(\vec{a},\vec{a}\right)}=\sqrt{\vec{a}\cdot\vec{a}}
\end{equation*}
Anmerkung: Das ist eine von unendlich vielen möglichen Norm-Definitionen\\
Einheitsvektor, normierter Vektor $\hat{a}: ||\hat{a}|| = 1$
\begin{equation*}
    \hat{a}=\frac{\vec{a}}{||\vec{a}||}
\end{equation*} 
für $\vec{a} \neq \vec{o}$\\
Winkel zwischen zwei Vektoren
\begin{equation*}
    \cos\gamma = \frac{\varphi \left(\vec{a},\vec{b}\right)}{||\vec{a}||\cdot||\vec{b}||}=\frac{\vec{a}\vec{b}}{||\vec{a}||\cdot||\vec{b}||}
\end{equation*}
Zwei Vektoren sind orthogonal, wenn $\varphi\left(\vec{a},\vec{b}\right)=\vec{a}\vec{b}=0$ gilt:\\
Spezialfall $\vec{o}\vec{a}=0$ für alle $\vec{a}$\\
Anmerkung: Bisher war der Betrag des Vektors $\vec{a}\left(x_a,y_a\right)$ gegeben als:\\\\
$|\vec{a}|=\sqrt{x_a^2+y_a^2+z_a^2}$\\\\
Warum als $||\vec{a}||$?\\\\
Verallgemeinerung auf Funktionen erfordert eine Unterscheidung.\\
Betragsbildung:
\begin{equation*}
    f(x)=-x^2, |f|=|-x^2|=x^2; x \in \mathbb{R}
\end{equation*}
Normbildung:
\begin{equation*}
    f(x)=-x^2, ||f||=\left[\int_{x_1}^{x_2} \left(-x^2\right)^2  \,dx \right]^\frac{1}{2}
\end{equation*}
\begin{equation*}
    =\left[\int_{x_1}^{x_2} x^4  \,dx \right]^\frac{1}{2} = \left[\frac{x^5}{5}|_{x_1}^{x_2}\right]^\frac{1}{2}=\mathrm{Zahl}
\end{equation*}
c) Def.: Ein Satz von Basisvektoren $\left\{\vec{e_i}\right\}$ heißt Orthonormalsystem (orthonomiert), wenn gilt:\\
\begin{equation*}
    \varphi\left(\hat{e_i}, \hat{e_j}\right) = \hat{e_i} \hat{e_j} = \delta_{ij} = 1 \left(i = j\right) \mathrm{bzw.} 0 \left(i \neq j\right)
\end{equation*}
Ist die Anzahl der Basisvektoren gleich der DImension des Vektorraums, so geiüt die Basis vollständig.\
(\underline{V}ollständig \underline{O}rtho\underline{N}ormal\underline{S}ystem, VONS)\\
Satz: Sind die $m \leq n$ Vektoren $\vec{e_1},\vec{e_2},\vec{e_3},\dots,\vec{e_m}$ in $\mathbb{V}^n$ paarweise zueinander orthogonal, so sind sie auch linear unabhängig. (Die Untersuchung des Satzes gilt nicht!)\\
Beispiel: Basisvektoren $\left\{\hat{i},\hat{j},\hat{k}\right\}$ des dreidimensionalen Raums: karthesisches Koordinatensystems\\
Jeder Vektor $\vec{a} \in \mathbb{V}$ lässt sich nach einem VONS entwickeln.
\begin{equation*}
    \vec{a}=\sum_{i=1}^{n}a_i\hat{e_i}=\left(a_1,a_2,\dots,a_n\right)
\end{equation*}
$\vec{a}=\left(\begin{array}{c}
    3 \\ 2 \\ 1
\end{array}\right)$ nach $\hat{i},\hat{j},\hat{k}$ entwickeln: $\vec{a}=3\hat{i}+2\hat{j}+\hat{k}$
Komponentendarstellung des Skalarprodukts (in einem Orthonormalsystem)\\
\begin{equation*}
    \vec{a}\cdot\vec{b}=\left(\sum_{i=1}^{n}a_i\hat{e_i}\right)\left(\sum_{j=1}^{n}b_j\hat{e_j}\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}a_ib_j(\hat{e_i}\hat{e_j}) = \sum_{i=1}^{n}a_ib_i
\end{equation*}
Wobei $\hat{e_i}\hat{e_j} = \delta_{ij}$
c) Konsturktion eines ONS aus linear unabhängigen Vektoren\\
Das Gramschmidsche Orthogonalitätsverfahren.\\\\
gegeben: linear unabhängiger, jedoch nicht notwendigerweise normierter und nicht notwendigerweise orthogonale Vektoren $\left\{\vec{a_i}\right\}$.\\\\
gesucht:ONS im von $\left\{\vec{a_i}\right\}$ aufgespannten Vektorraum.\\\\
Vorgehen:\\
\begin{itemize}
    \item[1)] Normierung von $\vec{a_1}: \hat{a_1}=\frac{\vec{a_1}}{||\vec{a_1}||}$
    \item[2)] Setze $i=2$
    \item[3)] Orthogonalisierung von $\vec{a_i}$ auf alle $\vec{a_j}$ mit $j < i$\\\\
                $\vec{a_1}'=\vec{a_i}-\sum_{j=1}^{i-1}\left(\hat{a_j}\vec{a_i}\right)\hat{a_j}$\\\\wobei $\hat{a_j}\vec{a_i}$ die Komponente von $\vec{a_i}$ in Richtung von $\hat{a_j}$ ist
\begin{center}
    Notiz: \textbf{Das Skalarprodukt eines Vektors $\vec{\mathbf{r}}$ mit einem Einheitsvektor $\hat{\mathbf{e}}$ ergibt die Projektion von $\vec{\mathbf{r}}$ auf die Richtung von $\hat{\mathbf{e}}$}
\end{center}
    \item[4)] Normierung von $\vec{a_i}': \hat{a_i}=\frac{\vec{a_i}'}{||\vec{a_i}||}$
    \item[5)] Erhöhe $i$ um 1 für $i < n$ und gehe zurück zu 3), ansonsten fertig, wenn $i=n$ 
\end{itemize}
Beispiel:\\
VONS aus
\begin{equation*}
    \left\{
    \left(\begin{array}{c}
        1 \\ 1 \\ 0
    \end{array}\right) + \left(\begin{array}{c}
        1 \\ 0 \\ 1
    \end{array}\right) + \left(\begin{array}{c}
        0 \\ 1 \\ 1
    \end{array}\right)
    \right\} = \left\{\vec{a_1},\vec{a_2},\vec{a_3}\right\}
\end{equation*}
Lineare Unabhängigkeit, da das Bilden der Determinanten $\neq 0$ als Ergebnis liefert.\\
\begin{itemize}
    \item[1)] Normierung von $\vec{a_1}: \hat{a_1}=\frac{\vec{a_1}}{||\vec{a_1}||}=\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)$
    \item[2)] Orthogonalisierung von $\vec{a_2}$ auf $\hat{a_1}$:\\\\
    \begin{equation*}
        \left(\vec{a_2}\cdot\hat{a_1}\right) = \frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)\left(\begin{array}{c}1 \\ 0 \\ 1\end{array}\right)=\frac{1}{}\sqrt{2}
    \end{equation*}
    \begin{equation*}
        \left(\vec{a_2}\cdot\hat{a_1}\right)\hat{a_1}=\frac{1}{\sqrt{2}}\cdot\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)=\frac{1}{2}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)
    \end{equation*}
    \begin{equation*}
        \vec{a_2}'=\vec{a_2}-\left(\vec{a_2}\hat{a_1}\right)\hat{a_1}=\left(\begin{array}{c}1 \\ 0 \\ 1\end{array}\right)-\frac{1}{2}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)=\left(\begin{array}{c}\frac{1}{2} \\ -\frac{1}{2} \\ 1\end{array}\right) \approx \left(\begin{array}{c}1 \\ -1 \\ 2\end{array}\right)
    \end{equation*}
    \item[3)] Normierung von $\vec{a_2}'$\\\\$\hat{a_2}=\frac{\vec{a_2}'}{||\vec{a_2}'||}=\frac{1}{\sqrt{6}}\left(\begin{array}{c}1 \\ -1 \\ 2\end{array}\right)$
    \item[4)] Orthogonalisierung von $\vec{a_3}$ auf $\hat{a_1}$ und $\vec{a_2}$\\
        \begin{equation*}
            \vec{a_3}'=\vec{a_3}-\left(\vec{a_3}\hat{a_1}\right)\hat{a_1}-\left(\vec{a_3}\hat{a_2}\right)\hat{a_2}
        \end{equation*}
        \begin{equation*}
            \left(\vec{a_3}\hat{a_1}\right)=\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)\left(\begin{array}{c}0 \\ 1 \\ 1\end{array}\right)=\frac{1}{\sqrt{2}}
        \end{equation*}
        \begin{equation*}
            \left(\vec{a_3}\hat{a_1}\right)\hat{a_1}=\frac{1}{\sqrt{2}}\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)=\frac{1}{2}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)
        \end{equation*}
        \begin{equation*}
            \left(\vec{a_3}\hat{a_2}\right)=\frac{1}{\sqrt{6}}\left(\begin{array}{c}1 \\ -1 \\ 2\end{array}\right)\left(\begin{array}{c}0 \\ 1 \\ 1\end{array}\right)=\frac{1}{\sqrt{6}}\left(-1+2\right)=\frac{1}{\sqrt{6}}
        \end{equation*}
        \begin{equation*}
            \left(\vec{a_3}\hat{a_2}\right)\hat{a_2}=\frac{1}{\sqrt{6}}\frac{1}{\sqrt{6}}\left(\begin{array}{c}1 \\ -1 \\ 2\end{array}\right)=\frac{1}{6}\left(\begin{array}{c}1 \\ -1 \\ 2\end{array}\right)
        \end{equation*}
        \begin{equation*}
            \vec{a_3}'=\left(\begin{array}{c}0 \\ 1 \\ 1\end{array}\right)-\frac{1}{2}\left(\begin{array}{c}1 \\ 1 \\ 0\end{array}\right)-\frac{1}{6}\left(\begin{array}{c}1 \\ -1 \\ 2\end{array}\right)=\left(\begin{array}{c}-\frac{4}{6} \\ \frac{4}{6} \\ \frac{4}{6}\end{array}\right)=\left(\begin{array}{c}1 \\ -1 \\ -1\end{array}\right)
        \end{equation*}
    \item[5)] Normierung von $\vec{a_3}': \hat{a_3}=\frac{\vec{a_3}'}{||\vec{a_3}'||}=\frac{1}{\sqrt{3}}\left(\begin{array}{c}1 \\ -1 \\ -1\end{array}\right)$
    \item[Test:] \begin{equation*}
        \det\left(\hat{a_1},\hat{a_2},\hat{a_3}\right)=\frac{1}{\sqrt{2}}\frac{1}{\sqrt{6}}\frac{1}{\sqrt{3}}\begin{vmatrix} 

            1 & 1  & 1 \\ 
    
            1 & -1 & -1 \\ 
    
            0 &  2 & 1 \\  
    
        \end{vmatrix}=\frac{1}{6}\left(1+2+2+1\right)=1
    \end{equation*} 
    Das Ergebnis eines VONS-Rechtssystemes ist \textbf{\underline{IMMER}} 1!
\end{itemize}
\subsection{Lineare Operatoren}
\subsubsection{Grundlagen}
Def.: Ein Operator $A$ ist eine (Rechen-) Vorschrif, durch welche einem Vektor $|a\rangle $ (einer Funktion $a$) ein Vektor $|b\rangle $ (eine Funktion b) zugeorddnet werden kann.
\begin{equation*}
    A|a\rangle  = |b\rangle 
\end{equation*}
Für einen linearen Operator gilt:
\begin{equation*}
    A\left(|a\rangle  + |b\rangle \right)=A|a\rangle + A|b\rangle 
\end{equation*}
\begin{equation*}
    A\left(\alpha|a\rangle \right) = \alpha \left(A|a\rangle \right)
\end{equation*}
Dies gilt für alle $|a\rangle , |b\rangle  \in \mathbb{V}$ und $\alpha \in \mathbb{R}$\\
Satz: Ist $A, B$ linear, so ist auch $(A+B), (A\cdot B)$ linear\\
Beispiele: Translation, Rotation, Inversion, $\dots$
\subsubsection{Verknüpfung von Operatoren}
\begin{itemize}
    \item[a)] Summe: $(A+B)|a\rangle = A|a\rangle + B|b\rangle$
    \item[b)] Produkt: $(A\cdot B)|a\rangle = A(B|a\rangle) \neq B(A|a\rangle)$\\Im Allgemeinen Nichtvertauschbarkeit von A und B
\end{itemize}
Beispiel:
\begin{itemize}
\item[A:] "Quadrieren", Skalarproduktbildung mit sich selbst.
\item[B:] Multiplizieren mit $\alpha \in \mathbb{R}$
\end{itemize}
\begin{equation*}
    AB|a\rangle = A|\alpha a\rangle = \langle a\alpha|\alpha a\rangle = \alpha^2 \langle a|a\rangle 
\end{equation*}
\begin{equation*}
    BA|a\rangle = B\langle a| a\rangle = \alpha \langle a|a\rangle 
\end{equation*}
Das heißt: $(AB-BA)|a\rangle \neq 0$\\
Definition $[A,B] = [A,B]_- = AB - BA$ Kommutator\\
\hspace*{2.9cm} $[a,B]_+ = AB + BA$ Antikommutator\\\\
Zwei Operatoren heißen vertauschbar (kommutieren), wenn gilt: $[A,B] = 0$\\\\
Beispiel:
\begin{itemize}
    \item[A:] Drehung um $\frac{\pi}{2}$ (gegen den Uhrzeigersinn)
    \item[B:] Spiegelung an $y = 0$ ($x$-Achse)
\end{itemize}

\subsection{Matrizen}
\subsubsection{Matrixdarstellung von Operatoren}
\begin{itemize}
    \item[Gegeben:] VONS $\left\{|\hat{e}_i\rangle\right\}$, linearer Operator $A$
    \item[] Alle Informationen über die Wirkungsweise von $A$ steckt in den den Bildvektoren $\left\{|A\hat{e}_i\rangle\right\}$!
\end{itemize}
\begin{equation*}
    A|a\rangle = A\sum_{i=1}^{n}a_i|\hat{e}_i\rangle = \sum_{i=1}^{n} a_i(A|\hat{e}_i\rangle) = |b\rangle
\end{equation*}
quadratisches Zahlenschema mit $A|\hat{e_i}\rangle$ als $i$-te Spalte \\
Matrixdarstellung von A in der Basis $\left\{|\hat{e}_i\rangle\right\}$ (Basisabhängig)\\\\
Es gilt:
\begin{equation*}
    |b\rangle = \sum_{k} b_k\vec{e}_k = A\sum_{l} a_l \vec{e}_l = \sum_{l} a_lA\vec{e}_l
\end{equation*}
Multipliziere mit $\vec{e}_j$ ergibt
\begin{equation*}
    \sum_{k}b_k\vec{e}_j\vec{e}_k = \sum_{l} a_l\vec{e}_jA\vec{e}_l = b_j = \sum_{l}A_{jl}a_l
\end{equation*}
$\vec{e}_j\vec{e}_k = \delta_{jk}$ und $\vec{e}_jA\vec{e}_l = A_{jl}$\\\\
Beispiel: Spiegelung an der $xz$-Ebene und die Streckung um den Faktor $2$\\
VONS $\left\{ \hat{i}, \hat{j}, \hat{k} \right\}$\\
\begin{itemize}
    \item[] $A\hat{i} = 2\hat{i}$
    \item[] $A\hat{j} = -2\hat{j}$
    \item[] $A\hat{k} = 2\hat{k}$
\end{itemize}
\begin{equation*}
    A = \left(\begin{array}{c c c}
        2 & 0 & 0\\
        0 & -2 & 0\\
        0 & 0 & 2\\
    \end{array}\right) = 2 \left(\begin{array}{c c c}
        1 & 0 & 0\\
        0 & -1 & 0\\
        0 & 0 & 1\\
    \end{array} \right)
\end{equation*}
Spiegelung des Vektors $\vec{a}=(1,2,3)$ an der $xz$-Ebene und Streckung um den Faktor 2.
\begin{equation*}
    A|a\rangle = \sum_{i} a_i\left(A|\hat{e}_i\rangle\right) \Rightarrow \left(A|a\rangle\right)_j = \sum_{i} a_iA_{ji}
\end{equation*}
Merkschema: Matrix $A$ mal Vektor $a$
\begin{equation*}
    A\vec{a} = \left(\begin{array}{c c c}
        2 & 0 & 0\\
        0 & -2 & 0\\
        0 & 0 & 2\\
    \end{array}\right) \cdot \left(\begin{array}{c}
        1\\
        2\\
        3\\
    \end{array}\right) = \left(\begin{array}{c c c}
        2\cdot 1 & 0\cdot 2 & 0\cdot 3\\
        0\cdot 1 & -2\cdot 2 & 0\cdot 3\\
        0\cdot 1 & 0\cdot 2 & 2\cdot 3\\
    \end{array}\right) = \left(\begin{array}{c}
        2\\
        -4\\
        6\\
    \end{array}\right)
\end{equation*}

\subsubsection{Addition und Multiplikation von Matrizen}
\textbf{(A,B sind Matrixdarstellungen linearer Operatoren)}\\
Addition:
\begin{equation*}
    (A+B)|a\rangle = A|a\rangle + B|a\rangle
\end{equation*}
$i$-te Komponente:\\
Linke Seite:
\begin{equation*}
    \left[\left(A+B\right)|a\rangle\right]_i = \sum_{j=1}^{n} \left(A+B\right)_{ij}a_j
\end{equation*}
Rechte Seite:
\begin{equation*}
    = \left[A|a\rangle\right]_i + \left[B|a\rangle\right]_i = \sum_{j=1}^{n} A_{ij}a_j + \sum_{j=1}^{n} B_{ij}a_j
\end{equation*}
Somit:
\begin{equation*}
    = \left(A_{ij}+B_{ij}\right)a_j
\end{equation*}
$\rightarrow$ Matrizen werden addiert (subtrahiert), indem man ihre Elemente addiert (subtrahiert):
\begin{equation*}
    (A\pm B)_{ij} = A_{ij} \pm B_{ij}
\end{equation*}
Beispiel:
\begin{equation*}
    \left(\begin{array}{c c}
        1 & 2\\
        3 & 4
    \end{array}\right) + \left(\begin{array}{c c}
        1 & 3\\
        2 & 4
    \end{array}\right) = \left(\begin{array}{c c}
        2 & 5\\
        5 & 8
    \end{array}\right)
\end{equation*}
Multiplikation:
\begin{equation*}
    (AB)|a\rangle = A(B|a\rangle)
\end{equation*}
$i$-te Komponente:
linke Seite:
\begin{equation*}
    \left[\left(AB\right)|a\rangle\right]_i = \sum_{j=1}^{n} \left(AB\right)_{ij}a_j=
\end{equation*}
rechte Seite:
\begin{equation*}
    \left[A\left(B\right)|a\rangle\right]_i = \sum_{l=1}^{n} A_{il}\left[B|a\rangle\right]_l = \sum_{l=1}^{n} A_{il}\sum_{j=1}^{n} B_{lj}a_j
\end{equation*}
\begin{equation*}
    = \sum_{j=1}^{n}\sum_{l}^{n}A_{il}B_{lj}a_j
\end{equation*}
Das $ij$-te Element der Produktmatrix ($AB$) ist das Skalarprodukt der $i$-ten Teile von $A$ und der $j$-ten Spalte von $B$.
\begin{equation*}
    \left(AB\right)_{ij} = \sum_{l=1}^{n}A_{il}B_{li}
\end{equation*}
Es ist keine Matrixdivision definiert - wohl aber eine Inversion $A^{-1}$\\
Beispiele: Pauli-Spinmatrizen
\begin{equation*}
    \sigma_x = \left(\begin{array}{c c}0&1\\1&0\end{array}\right), \sigma_y = \left(\begin{array}{c c}0&-i\\i&0\end{array}\right), \sigma_z = \left(\begin{array}{c c}1&0\\0&-1\end{array}\right)
\end{equation*}
\begin{equation*}
    \sigma_x\sigma_y = \left(\begin{array}{c c}0&1\\1&0\end{array}\right) \cdot \left(\begin{array}{c c}0&-i\\i&0\end{array}\right) = \left(\begin{array}{c c}-i&0\\0&i\end{array}\right)
\end{equation*}
\begin{equation*}
    \sigma_y\sigma_x = \left(\begin{array}{c c}0&-i\\i&0\end{array}\right) \cdot \left(\begin{array}{c c}0&1\\1&0\end{array}\right) = \left(\begin{array}{c c}-i&0\\0&i\end{array}\right)
\end{equation*}
Beispiel: Aus $A^2$ folgt nicht $A = 0$!
\begin{equation*}
    A = \left(\begin{array}{c c}
        0 & \alpha\\0 & 0
    \end{array}\right)
\end{equation*}

\subsection{Matrizen mit speziellen Eigenschaften}
\subsubsection{Die Einheitsmatrix $E$ (auch $I, 1$)}
\begin{equation*}
    E_{ij}=\delta_{ij}=\begin{array}{c}
        0 \text{ für } i \neq j\\1 \text{ für } i=j
    \end{array} = \left(\begin{array}{c c c}
        1&0&0\\0&1&0\\0&0&1
    \end{array}\right)
\end{equation*}
Es gilt: $AE = EA = A$ mit $E|a\rangle = |a\rangle$\\
$ij$-tes Element
\begin{equation*}
    (AE)_{ij}=\sum_{l=1}^{n}A_{il}\delta_{lj}=A_{ij}
\end{equation*}
\begin{equation*}
    (EA)_{ij}=\sum_{l=1}^{n}\delta_{il}A_{lj}=A_{ij}
\end{equation*}
$i$-te Komponente
\begin{equation*}
    (E|a\rangle)_i = \sum_{j=1}^{n} E_{lj}a_j = \sum_{j=1}^{n}\delta_{ij}a_j=a_i
\end{equation*}
\subsubsection{Digitalmatrix $D$}
Besonderheit: $AB =BA$, wenn A und B beide diagonal sind.\\
Die \underline{Spur} einer Matrix ist die Summe ihrer Diagonalelemente.
\begin{equation*}
    Sp(D) = \sum_{i}d_i, Sp(E) = n
\end{equation*}
allgemein:
\begin{equation*}
    Sp(A)=\sum_{i} A_{ii}
\end{equation*}
\begin{equation*}
    Sp(AB) = Sp(BA)
\end{equation*}
Es gilt: $D^m$ ist die Matrx im $(D^m)_{ij} = d_i^m\delta_{ij}$ für $, \in \mathbb{R}$\\
Beispiel:
\begin{equation*}
    (D^2)_{ij} = \sum_{l=1}^{n}D_{il}D_{lj}=\sum_{l=1}^{n}d_id_j\delta_{il}\delta_{lj}=d^2_i\delta_{ij}
\end{equation*}
\subsubsection{Die inverse Matrix}
Definition:
\begin{equation*}
    A\cdot A^{-1} = A^{-1}\cdot A = E
\end{equation*}
$A^{-1}$ repräsentiert diejenige Operation, die die durch $A$ repräsentierte Operation rückgängig macht\\
$A^{-1}$ existiert, falls $\det(A)\neq 0$ gilt: $A$ heißt dann reguläre Matrix und ist invertierbar (sonst ist $A$ singulär)\\
Beispiel: Inverse Matrix von $A=\begin{array}{c c}1&0\\-1&1\end{array}$\\
$\det(A)=1$, d.h. $A$ ist regulär, $A^{-1}$ existiert.
\begin{equation*}
    AA^{-1}=\left(\begin{array}{c c}1&0\\-1&1\end{array}\right)\cdot\left(\begin{array}{c c}a&b\\c&d\end{array}\right) = \left(\begin{array}{c c}1&0\\0&1\end{array}\right) = E
\end{equation*}
1)
\begin{equation*}
    a = 1, -a+c = 0 \rightarrow c=1
\end{equation*}
2)
\begin{equation*}
    b = 0, -b + d = 1 \rightarrow d=1
\end{equation*}
somit:
\begin{equation*}
    A^{-1} =\left(\begin{array}{c c}1&0\\1&1\end{array}\right)
\end{equation*}
\subsubsection{Die transponierte Matrix $\tilde{A}, A^T, A^+$}
Def.:
\begin{equation*}
    (\tilde{A})_{ij} = A_{ji}
\end{equation*}
$\tilde{A}$ entsteht aus $A$ durch Vertauschen von Zeilen und Spalten bzw. durch Spiegelung der Matrix an der Hauptdiagonalen\\
Def.: Symmetrische Matrix: $\tilde{A} = A$\
Beispiel:
\begin{equation*}
    A = \left(\begin{array}{c c c}
        1 & 0 & -1\\ 0 & 2 & o\\ -1 & i & 3
    \end{array}\right)
\end{equation*}
Def.: Orthogonale Matrix $\tilde{A} = A^{-1}$\\
Beispiel:
\begin{equation*}
    A = \left(\begin{array}{c c c}
        0&0&1\\1&0&0\\0&-1&0
    \end{array}\right)
    ,\, \tilde{A} = \left(\begin{array}{c c c}
        0&1&0\\0&0&-1\\1&0&0
    \end{array}\right)
\end{equation*}
Eigenschaften orthogonaler Matrizen:\\
\textbf{1.}
\begin{equation*}
    AA^{-1} = E = A\tilde{A}
\end{equation*}
Wobei $A\tilde{A}$ das Skalarprodukt der $i$-ten und $j$-ten Teilen von $A$ ergibt $\delta_{ij}$, das heißt Zeilenvektoren sind othonormiert.\\
\textbf{2.}
\begin{equation*}
    1 = \det(E) = \det(AA^{-1}) =^*\det(A)\det(A^{-1}) = \det(A) \det(\tilde{A}) = \left[\det(A)\right]^2
\end{equation*}
* - Eigenschaften von Determinanten $\det$:
\begin{itemize}
    \item $\det(A) = \det(\tilde{A})$
    \item $\det(AB) = \det(A)\cdot\det(B)$
\end{itemize}
Es muss gelten:\\
\begin{equation*}
    \det(A) = \pm 1,\, \text{falls} A\, \text{eine orthogonale Matrix ist}
\end{equation*}
Ganz allgemein:
\begin{equation*}
    (AB)^T = B^TA^T
\end{equation*}
\subsubsection{Die adjungierte Matrix $A^+$}
Erweiterung der Transporniertenbildung auf komplexe Matrizen\\
Def.:
\begin{equation*}
    A^+ = (\tilde{A})^*
\end{equation*}
Def.: selbstadjungierte Matrix (hermitische Matrix)
\begin{equation*}
    A^+ = A
\end{equation*}
(entspircht symmetrischen Matrizen im Komplexen)\\
Beispiel: alle quantenmechanische Operatoren sind hermitische
\begin{equation*}
    s_y = \frac{1}{2}\sigma_y = \frac{1}{2}\left(\begin{array}{c c}
        0&-i\\i&0
    \end{array}\right)
\end{equation*}
\begin{equation*}
    s_y^+ = \frac{1}{2}\left(\begin{array}{c c}0&-i\\i&0\end{array}\right) = \frac{1}{2}\left(\begin{array}{c c}0&-i\\i&0\end{array}\right)^* = \frac{1}{2}\left(\begin{array}{c c}0&-i\\i&0\end{array}\right) = s_y
\end{equation*}
Def.: Unitäre Matrizen $A^+ = A^-1$ entsprechen orthogonalen Matrizen im komplexen Zahlenraum
\subsection{Determinanten}
\subsubsection{Definition}
Die Determinante einer quadratischen Matrix $A$ der Dimension $n$ ist eine Zahl, welche durch Anwendung der folgenden Rechenvorschrift bestimmt wird.
\begin{equation*}
    \det(A) = \left\lvert \begin{array}{c c c}A_{11}&A_{12}&\dots\\A_{21}&A_{22}&\dots\\\vdots&\vdots&\ddots\end{array}\right\rvert =^{Def} \sum_{p}(-1)^{j(p)}\cdot A_{1i_1}\cdot A_{2i_2} \cdot A_{3i_3} \cdot \dots \cdot A_{ni_n}
\end{equation*}
$\sum_{p}$ gibt somit die Summer über alle möglichen Permutationen der Zahlen $1,2,\dots,n$ an.\\
$j(p)$ ist die Anzahl der INversionen der Permutationen $p = \left(\begin{array}{c c c c}1&2&\dots&n\\i_1&i_2&\dots&i_n\end{array}\right)$.\\
Beispiel:
\begin{equation*}
    \left\lvert\begin{array}{c c c}3&1&2\\0&4&-1\\2&1&7\end{array} \right\rvert = \begin{array}{c c c}A_{1i_1} \cdot & A_{2i_2} \cdot & A_{3i_3}\\ (+)3 \cdot & 4 \cdot &7\\-3 \cdot & (-1) \cdot& 1\\-1 \cdot & 0 \cdot & 7\\(+)1 \cdot &(-1) \cdot& 2\\(+)2\cdot&0\cdot&1\\-2 \cdot&4\cdot&2\end{array} \begin{array}{c c c c}i_1 & i_2 & i_3 & j\\(1 & 2& 3) & 0\\(1 & 3&2) & 1\\(2&1&3)&1\\(2&3&1)&2\\(3&1&2)&2\\(3&2&1)&1\end{array}
\end{equation*}
\begin{equation*}
    84 + 2 - 0 - 2 + 0 - 16 = 69
\end{equation*}
Bemerkungen:\\
- Determinanten stellen eine Kerngröße für eine Matrix dar.\
\textbf{1.} Laplace'scher Entwicklungssatz:
\begin{itemize}
    \item Entwischlung nach der $i$-ten Zeile
\end{itemize}
\begin{equation*}
    \det(A)=\sum_{k=1}^{n} (-1)^{i+k} A_{ik} D_{ik}
\end{equation*}
Wobei $D_{ik}$ eine Unterdeterminante ist das heißt, Determinante der $(n-1)\times (n-1)$ Matrix, die entsteht, wenn von $A$ die $i$-te Zeile und $k$-te Spalte gestrichen wird.\\
\textbf{2.} Entwicklung nach der Spalte
\begin{equation*}
    det(A) = \sum_{i=1}^{n}(-1)^{i+k}A_{ik}D_{ik}
\end{equation*}
Wiederholte Anwendung des Entwicklungssatzes: Zurückführen auf Unterdeterminanten 3.Ordnung und 2. Ordnung
\begin{equation*}
    \left\lvert \begin{array}{c c c}a&b&c\\d&e&f\\g&h&i\end{array} \right\rvert = a \left\lvert \begin{array}{c c}e&f\\h&i\end{array} \right\rvert - b \left\lvert \begin{array}{c c}d&f\\g&i\end{array} \right\rvert - c \left\lvert \begin{array}{c c}d&e\\g&h\end{array} \right\rvert = aei + bfg + cdh - ceg - bdi - afh
\end{equation*}
\begin{equation*}
    \left\lvert \begin{array}{c c}a&b\\c&d\end{array} \right\rvert = ad-bc
\end{equation*}
Beispiel:
\begin{equation*}
    \left\lvert\begin{array}{c c c c}(+)2&7&1&4\\(-)5&(+)0&(-)0&(+)3\\-1&2&1&0\\4&5&-1&1\end{array} \right\rvert = -5 \left\lvert\begin{array}{c c c}7&1&4\\2&1&0\\5&-1&1\end{array} \right\rvert + 3 \left\lvert\begin{array}{c c c}2&7&1\\-1&2&1\\4&5&-1\end{array} \right\rvert
\end{equation*}
Hier wurde nach der 2. Zeile entwickelt, da nur 2 Unterdeterminanten notwendig sind.\\
\begin{equation*}
    -5(7-8-20-2)+3(-4+28-5-8-10-7) = 97
\end{equation*}
Spezialfall: Determinante einer Dreiecksmatrix ist das Produkt der Diagonalelemente
\begin{equation*}
    \left\lvert \begin{array}{c c c c}1&0&0&0\\3&2&0&0\\7&-1&3&0\\5&0&6&4\end{array} \right\rvert = 4 \left\lvert \begin{array}{c c c}1&0&0\\3&2&0\\7&-1&3\end{array} \right\rvert = 4 \cdot 3 \left\lvert \begin{array}{c c}1&0\\3&2\end{array} \right\rvert = 4 \cdot 3 \cdot 2 \cdot 1 = 24
\end{equation*}
Hier wurde jeweils nach der letzten Spalte entwickelt.
\subsubsection{Eigenschaften von Determinanten}
\begin{itemize}
    \item[a)] Eine Determinante verschwindet (hat den  Wert 0), wenn Zeilen (doer Spalten) linear abhängig (oder identisch) sind.
\end{itemize}
Beweis: Laplace-Entwicklung bis zur Unterdeterminante 2.Ordnung
\begin{equation*}
    \left\lvert \begin{array}{c c}\lambda a & \lambda b\\a&b\end{array}\right\rvert = \lambda ab - \lambda ab = 0
\end{equation*}
\begin{equation*}
    \left\lvert \begin{array}{c c} \lambda a & a\\ \lambda b & b\end{array} \right\rvert = \lambda ab - \lambda ab = 0
\end{equation*}

\begin{itemize}
    \item[b)] Die  Bildung der transponierten Matrix ändert nicht den Wert der Determinante.
\end{itemize}
Beweis: zeilen bzw. Spalten spielen bei der Definition der Determinante bzw. im Laplace-Entwicklungssatz dieselbe Rolle.
\begin{equation*}
    \left\lvert \begin{array}{c c}a&b\\c&d\end{array}\right\rvert = ad-bc,\, \left\lvert \begin{array}{c c}a&c\\b&d\end{array}\right\rvert = ad - bc
\end{equation*}
\begin{itemize}
    \item[c)] Ein gemeinsamer Faktor in \underline{einer} Zeile (Spalte) kann vor die Determinante gezogen werden.
\end{itemize}
Beweis: Entwicklung nach der Zeile (Spalte), aus der $\lambda$ herausgezogen werden soll:
\begin{equation*}
    \sum_{k=1}^{n}(-1)^{i+k}(\lambda A_{ik})D_{ik} = \lambda \sum_{k=1}^{n}(-1)^{i+k}A_{ik}D_{ik}\, \text{ für } i \text{-te Zeile}
\end{equation*}
\begin{itemize}
    \item[d)] Die Vertauschung zweier Zeilen bzw. Spalten ändert das Vorzeichen der Determinanten
\end{itemize}
Beweis: Rückführung auf Unterdeterminante 2.Ordnung mit dem Laplace-Entwicklungssatz, wobei diese die Elemente der zu vertauschenden Zeilen (Spalten) enthalten.
\begin{equation*}
    \left\lvert \begin{array}{c c}a&b\\c&d\end{array}\right\rvert = ad-cb = -(bc-ad) - \left\lvert \begin{array}{c c}c&b\\a&b\end{array} \right\rvert=-\left\lvert \begin{array}{c c}b&a\\d&c\end{array}\right\rvert
\end{equation*}
\begin{itemize}
    \item[e)] Zeilen bzw. Spalten in Determinanten können beliebig linearkombiniert werden, ohne dass sich der Wert der Determinanten ändert.
\end{itemize}
\begin{equation*}
    \left\lvert \begin{array}{c c}a & b\\c+\lambda a & d+\lambda b\end{array} \right\rvert = ad+\lambda ab - bc -\lambda ab = ad - bc
\end{equation*}
\begin{itemize}
    \item[f)] Multiplikationssatz für Determinanten: $\det(AB) = \det(A)\det(B)$
\end{itemize}
\begin{equation*}
    \det(A)\det(B) = \left(\begin{array}{c c}ae + bg& af + bh\\ce + dg & cd + dh\end{array}\right) = AB
\end{equation*}
\begin{equation*}
    \det(AB)=(ae+bg)(cf+dh)-(ce+dg)(af+bh)=acef+adeh+befg+bdgh-acef-bceh-adfg-bdgh
\end{equation*}
\begin{equation*}
    = (ad-cb)(eh-fg) = \det(A)\det(B)
\end{equation*}
Warnung: Es gilt \underline{nicht}: $\det(A+B) = \det(A)+\det(B)$\\
\begin{eqnarray*}
    \det(A+A) = \det(2A) = 2^n\det(A)
\end{eqnarray*}
Wobei $n$ die Dimension der Matrix ist, der Faktor 2 kann aus jeder Zeile/Spalte herausgezogen werden.\\
\begin{equation*}
    \det(A^2)=\det(A\cdot A)=\det(A) \det(A) = \left[\det(A)\right]^2
\end{equation*}
Anwendung:
\begin{eqnarray*}
    1 = \det(E) = \det(AA^{-1}) = \det(A)\det(A^{-1})\\
    \Rightarrow \det(A^{-1})\frac{1}{\det(1)}
\end{eqnarray*}
Das heißt: $\det(A) \neq 0$ muss gelten, damit $\det(A^{-1})$ existiert.

\subsection{Lineare Gleichungssysteme}
\subsubsection{Gaußsche Algorithmen}
Lineares GLeichungssystem ($n$-Gleichungen für $n$ Unbekannte $x_i$)\\
\begin{eqnarray*}
    A_{11}x_1 + A_{12} x_2 + \dots + A_{1n} x_n = b_1\\
    A_{21}x_1 + A_{22} x_2 + \dots + A_{2n} x_n = b_2\\
    A_{31}x_1 + A_{32} x_2 + \dots + A_{3n} x_n = b_3\\
    \vdots\\\
    A_{n1}x_1 + A_{n2} x_2 + \dots + A_{nn} x_n = b_n\\
\end{eqnarray*}
Sas System heißt linear, weil alle $x_i$ nur in 0-ter Ordnung oder 1-ter Potenz vorkommen.\\
Alle $A_{ij}$ und $b_i$ sind bekannt.\\
In Matrixschreibweise:
\begin{equation*}
    A\vec{x} = \vec{b}
\end{equation*}
Homogenes Gleichungssystem: $\vec{b} = \vec{o}$\\
Inhomogenes Gleichungssystem: $\vec{b} \neq \vec{o}$\\\\
Achtung: Ein lineares Gleichungssystem ist nicht zwangsläufig lösbar!\\
\begin{itemize}
    \item[a)] Lösung mit der inversen Koeffizientenmatrix\\
    \begin{equation*}
        A^{-1}\,|\,A\vec{x} = \vec{b} \rightarrow \vec{x} = A^{-1}\vec{b}
    \end{equation*}\\
    Die Methode versagt, wenn $A^{-1}$ nicht existiert, es muss gelten: $\det(A) \neq 0$, $A$ muss eine reguläre Matrix sein.
    \item[b)]Cramersche Regel\\
    \begin{equation*}
        x_k = \frac{\det(A_k)}{\det(A)}
    \end{equation*}\\
    $A_k$: Matrix $A$, in der die $k$-te Spalte durch $\vec{b}$ ersetzt wurde.\\
    Die Methode versagt, wenn $\det(A) = 0$.
    \item[c)]Gaußscher Algorithmus (Methode der Wahl)\\
    Erweitertes Koeffizientenschema
    \begin{eqnarray*}
        A_{11} A_{12} \dots A_{1n} | b_1\\
        A_{21} A_{22} \dots A_{2n} | b_2\\
        \vdots\\
        A_{n1} A_{n2} \dots A_{nn} | b_n
    \end{eqnarray*}
    Es sollte wieder ein 0-Dreieck geschaffen werden, um eine Rückrechnung zu ermöglichen.\\
    \begin{itemize}
        \item[$\alpha$)]Vertauschen von Zeilen (einzelner Gleichungen)
        \item[$\beta$)] Bilden von Linearkombinationen von Zeilen.
    \end{itemize}
\end{itemize}
Beispiel:
\begin{equation*}
    \left(\begin{array}{c c c}1&-1&0\\2&1&1\\1&1&1\end{array}\right) \vec{x} = \left(\begin{array}{c}1\\4\\3\end{array}\right)\\
\end{equation*}
Koeffizientenschema:
\begin{equation*}
    \begin{matrix}
        1&-1&0&|\,1\\
        2&1&2&|\,4\\
        1&1&1&|\,3
    \end{matrix}
\end{equation*}
(2) - (3)
\begin{equation*}
    \begin{matrix}
        1&-1&0&|\,1\\
        2&0&0&|\,1\\
        1&1&1&|\,3
    \end{matrix}
\end{equation*}
Mit Vertauschen:
\begin{equation*}
    \begin{matrix}
        2&0&0&|\,1\\
        1&-1&0&|\,1\\
        1&1&1&|\,3
    \end{matrix}
\end{equation*}
Hier wurde die Dreiecksform, bzw. das Nullerdreieck geschaffen.\\
Lösung des \underline{homogenen} Gleichungssystemes: Anzahl der Nullzeilen $d=0$
\begin{equation*}
    \begin{matrix}
        1&0&0&|\,0\\1&-1&0&|\,0\\1&1&1&|\,0
    \end{matrix}
\end{equation*}
$\Rightarrow \vec{x_{hom}} = \vec{o}$\\
Allgemein gilt:\\
\begin{itemize}
    \item Wenn das Koeffizientenschema in Dreiecksform keine Nullzeiele hat, dann ist $\vec{x_{hom}} = \vec{o}$ die einzige Lösung
\end{itemize}
Lösung des \underline{inhomogenen} Gleichungssystems: Anzahl Nullzeilen $d' = 0,\,d' = d$\\
\begin{equation*}
    \begin{matrix}
        1&0&0&|\,1\\1&-1&0&|\,1\\1&1&1&|\,3\\
    \end{matrix}
\end{equation*}
Für $d=d'$ hat das inhomogene Gleichungssystem nur eine eindeutige Lösung.\\\\
Beispiel 2:
\begin{equation*}
    \left(\begin{array}{c c c}2&-2&0\\2&1&1\\-1&1&0\end{array}\right) \vec{x} = \left(\begin{array}{c}2\\2\\-1\end{array}\right)
\end{equation*}
Koeffizientenschema:
\begin{equation*}
    \begin{matrix}
        2&-2&0&|\,2\\
        2&1&1&|\,2\\
        -1&1&0&|\,1
    \end{matrix}
\end{equation*}
\begin{equation*}
    \begin{matrix}
        0&0&0&|\,0\\
        -1&1&0&|\,-1\\
        2&1&1&|\,2
    \end{matrix}
\end{equation*}
Lösung des \underline{homogenen} Gleichungssystems: Anzahl der Nullzeilen $d=1$
\begin{equation*}
    \begin{matrix}
        0&0&0&|\,0\\
        -1&1&0&|\,0\\
        2&1&1&|\,0
    \end{matrix}
\end{equation*}
\underline{eine} Nullzeile: Wahl des Parameters $x_1 = \lambda$, dadurch: $x_2 = \lambda$, dadurch: $x_3 = -3 \lambda$
\begin{equation*}
    \vec{x_{hom}} = \lambda \left(\begin{array}{c}
        1\\1\\-3
    \end{array}\right)
\end{equation*}
Allgemein gilt: treten im Koeffizientenschema in Dreiecksform $d\neq 0$ Nullzeilen auf, so sind $d$ Unbekannte frei wählbar $\rightarrow$ $d$- parametrige Lösungsschar\\
Lösung des \underline{inhomogenen} Gleichungssystem: Anzahl der Nullzeilen $d' = 1$
\begin{equation*}
    \begin{matrix}
        0&0&0&|\,0\\
        -1&1&0&|\,-1\\
        2&1&1&|\,2\\
    \end{matrix}
\end{equation*}
\underline{eine} Nullzeile: Wahl des Parameters: $x_1 = \lambda$\\
\begin{eqnarray*}
    -\lambda + x_2 = -1 \Rightarrow x_2 = \lambda - 1\\
                        \Rightarrow x_3 = 3-3\lambda
\end{eqnarray*}
\begin{equation*}
    \vec{x_{ih}}=\left(\begin{array}{c}0\\-1\\3\end{array}\right)+\lambda\left(\begin{array}{c}1\\1\\-3\end{array}\right)=\vec{x_p}+\vec{x_{hom}}
\end{equation*}
Allgemein gilt:
\begin{itemize}
    \item[] Treten im erweiterten Koeffizientenschema $d' \neq 0$ Nullzeilen auf, so ist die allgemeine Lösung eine $d$-parametrige Schar.
\end{itemize}
Satz:
\begin{itemize}
    \item[] Die allgemeine Lösung $\vec{x_{ih}}$ eines inhomogenen linearen Gleichungssystems lässt sich als Summe aus der allgemeinen Lösung des homogenen Systems $\vec{x_{hom}}$ und einer partikulären Lösung (speziellen Lösung) des Systems schreiben.
\end{itemize}
Beweis:
\begin{eqnarray*}
    A\vec{x_{ih}}=\vec{b}\\
    A(\vec{x_{hom}} + \vec{x_p}) = A\vec{x_{hom}} + A\vec{x_p} = \vec{O}+\vec{b}
\end{eqnarray*}
\\
Beispiel 3:
\begin{equation*}
    \left(\begin{array}{c c c }
        2&-2&0\\2&1&1\\-1&1&0
    \end{array}\right) \vec{x} = \left(\begin{array}{c}1\\2\\-1\end{array}\right)
\end{equation*}
Koeffizientenschema:
\begin{equation*}
    \begin{matrix}
        2&-2&0&|\,1\\
        2&1&1&|\,2\\
        -1&1&0&|\,-1
    \end{matrix}
\end{equation*}
\begin{equation*}
    \begin{matrix}
        0&0&0&|\,-1\\
        -1&1&0&|\,-1\\
        2&1&1&|\,2
    \end{matrix}
\end{equation*}
Lösen des homogenen Gleichungssystems bleibt gleich wie bei Beispiel 2.\\\\
Lösen des inhomogenen Gleichungssystems:
\begin{equation*}
    \begin{matrix}
        0&0&0&|\,-1\\
        -1&1&0&|\,-1\\
        2&1&1&|\,2
    \end{matrix}
\end{equation*}
Da $0 \neq -1$ ist dies ein Widerspruch und somit nicht lösbar!\\\\
Allgemein glit:
\begin{itemize}
    \item[] Stimmen die Anzahlen der Nullzeilen im Koeffizientenschema ($d$) und im erweiterten Koeffizientenschme ($d'$) nicht überein ($d\neq d'$), so ist das Gleichungssystem nicht lösbar.
\end{itemize}
Rechenregeln für:
\begin{itemize}
    \item[a)] Matritzen
    \item[b)] Determinanten
    \item[c)] Koeffizientenschemata
\end{itemize}
\begin{itemize}
    \item[1)] Gemeinsame Faktoren
    \begin{itemize}
        \item[a)] Kann vorgezogen werden, wenn er in \underline{allen} Elementen der Matrix enthalten ist.
        \item[b)] Kann vorgezogen werden, wenn er in \underline{einer} Zeile oder Spalte enthalten ist.
        \item[c)] Kann gekürtzt werden, wenn er in \underline{allen} Elementen \underline{einer} Zeile oder Spalte enthalten ist.
    \end{itemize}
    \item[2)] Vertauschen von Zeilen bzw. Spalten
    \begin{itemize}
        \item[a)] Nicht erlaubt
        \item[b)] Die Vertauschung von Zeilen bzw. Spalten verändert das Vorzeichen
        \item[c)] Vertauschung von Zeilen ist erlaubt
    \end{itemize}
    \item[3)] Linearkombinationen von Zeilen bzw. Spalten
    \begin{itemize}
        \item[a)] nicht erlaubt
        \item[b)] erlaubt für Zeilen bzw. Spalten
        \item[c)] ist erlaubt für Zeilen 
    \end{itemize}
\end{itemize}

\subsection{Basistransformation}
Gegeben: linearer Vektorraum $\mathbb{V}$, Dimension $n$\\
Basissysteme: $\{ \vec{e_i} \}$ "alte Basis"\\
\hspace*{1cm} $\{ \vec{e'_i} \}$ "neue Basis"\\\\
Gesucht: Umrechnungsbeziehung zwischen dern Basissystemen sowie für die Matrix- und Komponentendarstellung von Operatoren bzw. Vektoren in den Basissystemen.

\subsubsection{Transformationsmatrix}
Komponentendarstellung der $\{\vec{e'_i}\}$ in $\{ \vec{e_i} \}$
\begin{equation*}
    \vec{e_i} = \sum_{j=1}^{n}C_{ji}\vec{e_j}=\left(\begin{array}{c}C_{1i}\\C_{2i}\\\vdots\\C_{ni}\end{array}\right)
\end{equation*}
Zusammenfassung der $C_{ji}$ als Matrix:
\begin{equation*}
    C = \left(\begin{array}{c c c c}
        C_{11}&C_{12}&\dots&C_{1n}\\
        C_{21}&C_{22}&\dots&C_{2n}\\
        \vdots&\vdots&\ddots&\vdots\\
        C_{n1}&C_{n2}&\dots&C_{nn}\\
    \end{array}\right)
\end{equation*}
Transformationsmatrix:\\
Spaltvektoren sind die neuen Basisvektoren ausgedrückt in der alten Basis:\\
$\{\vec{e'_1}\}$, $\{\vec{e'_2}\}$, $\{\vec{e'_3}\}$ in $\{\vec{e_i}\}$
\end{document}
